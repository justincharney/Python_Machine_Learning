{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Data"
      ],
      "metadata": {
        "id": "FNvzq21z4VBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# connect drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "#path to folder\n",
        "project_folder = Path('/content/gdrive/MyDrive/BIEN410_FinalProject')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHA42l74XTK",
        "outputId": "a5764c0f-8ad2-4007-e120-bcf52986eaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = project_folder/'input_file/infile.txt'\n",
        "\n",
        "#look at the text file\n",
        "with open(input_file, 'r') as f:\n",
        "  data = f.read(1000) # first 1000 characters\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCGPQEEz4g8d",
        "outputId": "1cb28b87-08c4-4f96-f60a-08c3abf5f9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">d1dlwa | 116\n",
            "SLFEQLGGQAAVQAVTAQFYANIQADATVATFFNGIDMPNQTNKTAAFLCAALGGPNAWTGRNLKEVHANMGVSNAQFTTVIGHLRSALTGAGVAAALVEQTVAVAETVRGDVVTV\n",
            ">d2bkma | 128\n",
            "EQWQTLYEAIGGEETVAKLVEAFYRRVAAHPDLRPIFPDDLTETAHKQKQFLTQYLGGPPLYTAEHGHPMLRARHLRFEITPKRAEAWLACMRAAMDEIGLSGPAREQFYHRLVLTAHHMVNTPDHLD\n",
            ">d1asha | 147\n",
            "ANKTRELCMKSLEHAKVDTSNEARQDGIDLYKHMFENYPPLRKYFKSREEYTAEDVQNDPFFAKQGQKILLACHVLCATYDDRETFNAYTRELLDRHARDHVHMPPEVWTDFWKLFEEYLGKKTTLDEPTKQAWHEIGREFAKEINK\n",
            ">d4hswa | 137\n",
            "GFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQELKSMAKFGDHTEKVFNLMMEVADRATDCVPLASDANTLVQMKQHSSLTTGNFEKFFVALVEYMRASGQSFDSQSWDRFGKNLVSALSSAGMK\n",
            ">d1x9fd | 140\n",
            "ECLVTESLKVKLQWASAFGHAHERVAFGLELWRDIIDDHPEIKAPFSRVRGDNIYSPEFGAHSQRVLSGLDITISMLDTPDMLAAQLAHLKVQHVERNLKPEFFDIFLKHLLHVLGDRLGTHFDFGAWHDCVDQIIDGIK\n",
            ">d1x9fc | 149\n",
            "HEHCCSEEDHRIVQKQWDILWRDTESSKIKIGFGRLLLTKLAKDIPEVNDLFKRVDIEHAEGPKFSAHALRILNGLDLAINLLDDPPALDAALDHLAHQHEVREGVQKAHFKKFGEILATGLPQVLDDYDALAWKSCLKGILTKISSRL\n",
            ">d1jl7a | 147\n",
            "GLSAAQRQVVASTWKDIAGADNGAGVGKECLSKFISAHPEMAAVFGFSGASDPGVAELGAKVLAQIGVAVSHLGDEGKM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the file based on new lines\n",
        "data = [x for x in open(input_file).read().splitlines()]\n",
        "#keep only the even entries (sequences) \n",
        "seqs = data[1::2]\n",
        "len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyNQX3E34ku2",
        "outputId": "c1c64e67-a21e-4bf1-e742-f70a66b91d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5326"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the targets/labels\n",
        "label_file = project_folder/'training_data/labels.txt'\n",
        "\n",
        "#look at the text file\n",
        "with open(label_file, 'r') as f:\n",
        "  data = f.read(1000) # first 1000 characters\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDgZA9644mf",
        "outputId": "b50d9822-c805-4d9c-d117-f591cf1be35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">d1dlwa | 116\n",
            "SLFEQLGGQAAVQAVTAQFYANIQADATVATFFNGIDMPNQTNKTAAFLCAALGGPNAWTGRNLKEVHANMGVSNAQFTTVIGHLRSALTGAGVAAALVEQTVAVAETVRGDVVTV\n",
            "-HHHHH--HHHHHHHHHHHHHHHHH----HHHH----HHHHHHHHHHHHHHHH----------HHHHH------HHHHHHHHHHHHHHHHHH---HHHHHHHHHHHHHHHHHH---\n",
            ">d2bkma | 128\n",
            "EQWQTLYEAIGGEETVAKLVEAFYRRVAAHPDLRPIFPDDLTETAHKQKQFLTQYLGGPPLYTAEHGHPMLRARHLRFEITPKRAEAWLACMRAAMDEIGLSGPAREQFYHRLVLTAHHMVNTPDHLD\n",
            "-----HHHHH-HHHHHHHHHHHHHHHHHH-----------HHHHHHHHHHHHHHHH----HHHHHH----HHHHHH-----HHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHH--------\n",
            ">d1asha | 147\n",
            "ANKTRELCMKSLEHAKVDTSNEARQDGIDLYKHMFENYPPLRKYFKSREEYTAEDVQNDPFFAKQGQKILLACHVLCATYDDRETFNAYTRELLDRHARDHVHMPPEVWTDFWKLFEEYLGKKTTLDEPTKQAWHEIGREFAKEINK\n",
            "-HHHHHHHHHHHHH------HHHHHHHHHHHHHHHHH-HHHHHH--------HHHHHH-HHHHHHHHHHHHHHHHHHHH---HHHHHHHHHHHHHHHHH------HHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHHHH-\n",
            ">d4hswa | 137\n",
            "GFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQELKSMAKFGDHTEKVFNLMMEVADRATDCVPLASDANTLVQMKQHSSLTTGNFEKFFVALVEYMRASGQSFDSQSWDRFGKNLVSALSSAGMK\n",
            "-HHHHHHHHHHHHHHHHH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the file based on new lines\n",
        "label_data = [x for x in open(label_file).read().splitlines()] \n",
        "labels = label_data[2::3]\n",
        "len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1y120zb47Sx",
        "outputId": "e1e87eae-95c3-463b-db20-237187fdfbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5326"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess data"
      ],
      "metadata": {
        "id": "fZY8zW5v5Kf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor():\n",
        "  def __init__(self):\n",
        "    self.seq_dct = None\n",
        "    self.label_dct = None\n",
        "    self.flatX = None\n",
        "    self.flatY = None\n",
        "    self.flatYoh = None\n",
        "\n",
        "  def encode(self, sequence_list:list, dct):\n",
        "    \"\"\" Takes in a list of sequences and returns a \n",
        "    list of sequences after numerically encoding\"\"\"\n",
        "    seqs2int = []\n",
        "    #nested = any(isinstance(i, list) for i in sequence_list) # check if list is nested or not\n",
        "    for seq in sequence_list:\n",
        "      ints=[]\n",
        "      for i in list(seq):\n",
        "        ints.append(list(dct.values()).index(i))\n",
        "      seqs2int.append(ints)\n",
        "    return seqs2int\n",
        "  \n",
        "  def flatten(self, l:list):\n",
        "    \"\"\"Takes in a list of lists and flattens it\"\"\"\n",
        "    return [item for sublist in l for item in sublist]\n",
        "  \n",
        "  def transformX(self, seqs:list, N=3):\n",
        "    \"\"\"Takes in a sequence string or list of strings and creates input for NB model\"\"\"\n",
        "    aas = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "    dct = dict(enumerate(aas)) # create a dictionary of ammino acids\n",
        "    self.seq_dct = dct\n",
        "    enc = self.encode(seqs, dct)\n",
        "    flat = self.flatten(enc)\n",
        "    self.flatX = flat\n",
        "    new_seq = flat.copy()\n",
        "    #pad with N-1 dummy tokens (20)\n",
        "    new_seq.extend([20]*(N-1))\n",
        "    toks = [new_seq[i:i+N] for i in range(len(new_seq)-N+1)]\n",
        "    return np.array(toks)\n",
        "  \n",
        "  def labeldict(self, y:list):\n",
        "    set_labels = set([])\n",
        "    for l in y:\n",
        "      for seq in l:\n",
        "        for char in list(seq):\n",
        "          set_labels.add(char.lower())\n",
        "    label2idx = {l: i for i, l in enumerate(list(set_labels))}\n",
        "    self.label_dct = label2idx\n",
        "    return label2idx\n",
        "\n",
        "  def encodeY(self, arr, dict):\n",
        "    seqs2int = []\n",
        "    for lst in arr:\n",
        "      ints = []\n",
        "      for i in list(lst):\n",
        "        ints.append(dict[i.lower()])\n",
        "      seqs2int.append(ints)\n",
        "    return seqs2int\n",
        "  \n",
        "  def one_hot(self, np_arr, categories=2):\n",
        "    cat_seqs = []\n",
        "    for idx, label in np.ndenumerate(np_arr):\n",
        "      cat_seqs.append(np.zeros(categories))\n",
        "      cat_seqs[-1][label] = 1.0\n",
        "    return np.array(cat_seqs)\n",
        "  \n",
        "  def transformY(self, labels:list):\n",
        "    label_dict = self.labeldict(labels)\n",
        "    enc_y = self.encodeY(labels, label_dict)\n",
        "    flat_y = self.flatten(enc_y)\n",
        "    self.flatY = flat_y\n",
        "    oh_y = self.one_hot(np.array(flat_y))\n",
        "    self.flatYoh = oh_y\n",
        "    return oh_y"
      ],
      "metadata": {
        "id": "vih7BEUR5Pet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = Preprocessor()\n",
        "X = pre.transformX(seqs)\n",
        "print(X.shape)\n",
        "y = pre.transformY(labels)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w0VZws15Rsc",
        "outputId": "413c7530-e912-40fe-9533-7aef0d1de969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1147861, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1147861, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#create training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "mFVrfCs25U3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNB Classifier"
      ],
      "metadata": {
        "id": "xgLqJDW05qUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNB():\n",
        "  def __init__(self):\n",
        "    self.log_probs = []\n",
        "    self.prior_probs = None\n",
        "    self.count_matrix = []\n",
        "    self.n_features = None\n",
        "    self.class_count = None\n",
        "    self.log_probs = []\n",
        "    self.classes = np.array([0,1])\n",
        "    \n",
        "  def fit(self, X, y, alpha=1e-3):\n",
        "    self.n_features = X.shape[1]\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    unique_categories = np.unique(X)\n",
        "\n",
        "    for i in range(self.n_features):\n",
        "      feature_count = []\n",
        "      feature = X[:,i]\n",
        "      for j in range(n_classes):\n",
        "        mask = y[:,j].astype(bool)\n",
        "        counts = np.bincount(feature[mask], minlength=len(unique_categories))\n",
        "        feature_count.append(counts)\n",
        "      self.count_matrix.append(np.array(feature_count))\n",
        "      self.class_count = y.sum(axis=0)\n",
        "    \n",
        "    for i in range(self.n_features):\n",
        "      num = self.count_matrix[i] + alpha\n",
        "      den = num.sum(axis=1).reshape(-1,1)\n",
        "      log_prob = np.log(num) - np.log(den)\n",
        "      self.log_probs.append(log_prob)\n",
        "    \n",
        "    self.prior_probs = np.log(self.class_count) - np.log(self.class_count.sum())\n",
        "\n",
        "  def predict(self, test_sample):\n",
        "    probs = np.zeros((1,2))\n",
        "\n",
        "    for i in range(self.n_features):\n",
        "      category = test_sample[i]\n",
        "      probs += self.log_probs[i][:, category]\n",
        "    \n",
        "    probs += self.prior_probs\n",
        "    return self.classes[np.argmax(probs)]"
      ],
      "metadata": {
        "id": "mTad5HX_5tWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnb = CNB()\n",
        "cnb.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "yIs-ss06585z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "\n",
        "for aa in X_test:\n",
        "  preds.append(cnb.predict(aa))"
      ],
      "metadata": {
        "id": "hleYXqW76Cfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Validation Accuracy: {(np.array(preds) == np.argmax(y_test, axis=1)).mean()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9vMbWPH6J4V",
        "outputId": "010c739e-b383-47bf-ed9e-26c9d09effee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6640153676608312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN"
      ],
      "metadata": {
        "id": "1c6bomb46Tis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#normalize input and test input\n",
        "X_train_norm = (X_train - np.mean(X_train, axis=0))/np.std(X_train, axis=0)\n",
        "X_test_norm = (X_test - np.mean(X_test))/np.std(X_test)\n",
        "\n",
        "y = np.argmax(y_train, axis=1)\n",
        "y_ = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "kmE4Q9P66XqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=90)\n",
        "neigh.fit(X_train_norm, y)\n",
        "preds = neigh.predict(X_test_norm)"
      ],
      "metadata": {
        "id": "_LllZwCH6vB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation accuracy: \", str(accuracy_score(preds, y_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly_E1-gG7asf",
        "outputId": "09d5eebe-cebb-454e-9239-c7d12811801b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.6697521049949253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HMM"
      ],
      "metadata": {
        "id": "LC-vmaCZ7iD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code reproduced from numpy ML library to train HMM: https://github.com/ddbourgin/numpy-ml"
      ],
      "metadata": {
        "id": "ghD7H42v9Fm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy ml\n",
        "class MultinomialHMM:\n",
        "    def __init__(self, A=None, B=None, pi=None, eps=None):\n",
        "        r\"\"\"\n",
        "        A simple hidden Markov model with multinomial emission distribution.\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : :py:class:`ndarray <numpy.ndarray>` of shape `(N, N)` or None\n",
        "            The transition matrix between latent states in the HMM. Index `i`,\n",
        "            `j` gives the probability of transitioning from latent state `i` to\n",
        "            latent state `j`. Default is None.\n",
        "        B : :py:class:`ndarray <numpy.ndarray>` of shape `(N, V)` or None\n",
        "            The emission matrix. Entry `i`, `j` gives the probability of latent\n",
        "            state i emitting an observation of type `j`. Default is None.\n",
        "        pi : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)` or None\n",
        "            The prior probability of each latent state. If None, use a uniform\n",
        "            prior over states. Default is None.\n",
        "        eps : float or None\n",
        "            Epsilon value to avoid :math:`\\log(0)` errors. If None, defaults to\n",
        "            the machine epsilon. Default is None.\n",
        "        Attributes\n",
        "        ----------\n",
        "        A : :py:class:`ndarray <numpy.ndarray>` of shape `(N, N)`\n",
        "            The transition matrix between latent states in the HMM. Index `i`,\n",
        "            `j` gives the probability of transitioning from latent state `i` to\n",
        "            latent state `j`.\n",
        "        B : :py:class:`ndarray <numpy.ndarray>` of shape `(N, V)`\n",
        "            The emission matrix. Entry `i`, `j` gives the probability of latent\n",
        "            state `i` emitting an observation of type `j`.\n",
        "        N : int\n",
        "            The number of unique latent states\n",
        "        V : int\n",
        "            The number of unique observation types\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(I, T)`\n",
        "            The collection of observed training sequences.\n",
        "        I : int\n",
        "            The number of sequences in `O`.\n",
        "        T : int\n",
        "            The number of observations in each sequence in `O`.\n",
        "        \"\"\"\n",
        "        eps = np.finfo(float).eps if eps is None else eps\n",
        "\n",
        "        # prior probability of each latent state\n",
        "        if pi is not None:\n",
        "            pi[pi == 0] = eps\n",
        "\n",
        "        # number of latent state types\n",
        "        N = None\n",
        "        if A is not None:\n",
        "            N = A.shape[0]\n",
        "            A[A == 0] = eps\n",
        "\n",
        "        # number of observation types\n",
        "        V = None\n",
        "        if B is not None:\n",
        "            V = B.shape[1]\n",
        "            B[B == 0] = eps\n",
        "\n",
        "        self.parameters = {\n",
        "            \"A\": A,  # transition matrix\n",
        "            \"B\": B,  # emission matrix\n",
        "            \"pi\": pi,  # prior probability of each latent state\n",
        "        }\n",
        "\n",
        "        self.hyperparameters = {\n",
        "            \"eps\": eps,  # epsilon\n",
        "        }\n",
        "\n",
        "        self.derived_variables = {\n",
        "            \"N\": N,  # number of latent state types\n",
        "            \"V\": V,  # number of observation types\n",
        "        }\n",
        "\n",
        "    def generate(self, n_steps, latent_state_types, obs_types):\n",
        "        \"\"\"\n",
        "        Sample a sequence from the HMM.\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_steps : int\n",
        "            The length of the generated sequence\n",
        "        latent_state_types : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
        "            A collection of labels for the latent states\n",
        "        obs_types : :py:class:`ndarray <numpy.ndarray>` of shape `(V,)`\n",
        "            A collection of labels for the observations\n",
        "        Returns\n",
        "        -------\n",
        "        states : :py:class:`ndarray <numpy.ndarray>` of shape `(n_steps,)`\n",
        "            The sampled latent states.\n",
        "        emissions : :py:class:`ndarray <numpy.ndarray>` of shape `(n_steps,)`\n",
        "            The sampled emissions.\n",
        "        \"\"\"\n",
        "        P = self.parameters\n",
        "        A, B, pi = P[\"A\"], P[\"B\"], P[\"pi\"]\n",
        "\n",
        "        # sample the initial latent state\n",
        "        s = np.random.multinomial(1, pi).argmax()\n",
        "        states = [latent_state_types[s]]\n",
        "\n",
        "        # generate an emission given latent state\n",
        "        v = np.random.multinomial(1, B[s, :]).argmax()\n",
        "        emissions = [obs_types[v]]\n",
        "\n",
        "        # sample a latent transition, rinse, and repeat\n",
        "        for i in range(n_steps - 1):\n",
        "            s = np.random.multinomial(1, A[s, :]).argmax()\n",
        "            states.append(latent_state_types[s])\n",
        "\n",
        "            v = np.random.multinomial(1, B[s, :]).argmax()\n",
        "            emissions.append(obs_types[v])\n",
        "\n",
        "        return np.array(states), np.array(emissions)\n",
        "\n",
        "    def log_likelihood(self, O):\n",
        "        r\"\"\"\n",
        "        Given the HMM parameterized by :math:`(A`, B, \\pi)` and an observation\n",
        "        sequence `O`, compute the marginal likelihood of `O`,\n",
        "        :math:`P(O \\mid A,B,\\pi)`, by marginalizing over latent states.\n",
        "        Notes\n",
        "        -----\n",
        "        The log likelihood is computed efficiently via DP using the forward\n",
        "        algorithm, which produces a 2D trellis, ``forward`` (sometimes referred\n",
        "        to as `alpha` in the literature), where entry `i`, `j` represents the\n",
        "        probability under the HMM of being in latent state `i` after seeing the\n",
        "        first `j` observations:\n",
        "        .. math::\n",
        "            \\mathtt{forward[i,j]} = P(o_1, \\ldots, o_j, q_j=i \\mid A, B, \\pi)\n",
        "        Here :math:`q_j = i` indicates that the hidden state at time `j` is of\n",
        "        type `i`.\n",
        "        The DP step is:\n",
        "        .. math::\n",
        "            \\mathtt{forward[i,j]}\n",
        "               &= \\sum_{s'=1}^N \\mathtt{forward[s',j-1]} \\cdot\n",
        "                   \\mathtt{A[s',i]} \\cdot \\mathtt{B[i,o_j]} \\\\\n",
        "               &= \\sum_{s'=1}^N P(o_1, \\ldots, o_{j-1}, q_{j-1}=s' \\mid A, B, \\pi)\n",
        "                    P(q_j=i \\mid q_{j-1}=s') P(o_j \\mid q_j=i)\n",
        "        In words, ``forward[i,j]`` is the weighted sum of the values computed on\n",
        "        the previous timestep. The weight on each previous state value is the\n",
        "        product of the probability of transitioning from that state to state `i`\n",
        "        and the probability of emitting observation `j` in state `i`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(1, T)`\n",
        "            A single set of observations.\n",
        "        Returns\n",
        "        -------\n",
        "        likelihood : float\n",
        "            The likelihood of the observations `O` under the HMM.\n",
        "        \"\"\"\n",
        "        if O.ndim == 1:\n",
        "            O = O.reshape(1, -1)  # noqa: E741\n",
        "\n",
        "        I, T = O.shape  # noqa: E741\n",
        "\n",
        "        if I != 1:  # noqa: E741\n",
        "            raise ValueError(\"Likelihood only accepts a single sequence\")\n",
        "\n",
        "        forward = self._forward(O[0])\n",
        "        log_likelihood = logsumexp(forward[:, T - 1])\n",
        "        return log_likelihood\n",
        "\n",
        "    def decode(self, O):\n",
        "        r\"\"\"\n",
        "        Given the HMM parameterized by :math:`(A, B, \\pi)` and an observation\n",
        "        sequence :math:`O = o_1, \\ldots, o_T`, compute the most probable\n",
        "        sequence of latent states, :math:`Q = q_1, \\ldots, q_T`.\n",
        "        Notes\n",
        "        -----\n",
        "        HMM decoding is done efficiently via DP using the Viterbi algorithm,\n",
        "        which produces a 2D trellis, ``viterbi``, where entry `i`, `j` represents the\n",
        "        probability under the HMM of being in state `i` at time `j` after having\n",
        "        passed through the *most probable* state sequence :math:`q_1,\\ldots,q_{j-1}`:\n",
        "        .. math::\n",
        "            \\mathtt{viterbi[i,j]} =\n",
        "                \\max_{q_1, \\ldots, q_{j-1}}\n",
        "                    P(o_1, \\ldots, o_j, q_1, \\ldots, q_{j-1}, q_j=i \\mid A, B, \\pi)\n",
        "        Here :math:`q_j = i` indicates that the hidden state at time `j` is of\n",
        "        type `i`, and :math:`\\max_{q_1,\\ldots,q_{j-1}}` represents the maximum over\n",
        "        all possible latent state sequences for the first `j-1` observations.\n",
        "        The DP step is:\n",
        "        .. math::\n",
        "            \\mathtt{viterbi[i,j]} &=\n",
        "                \\max_{s'=1}^N \\mathtt{viterbi[s',j-1]} \\cdot\n",
        "                    \\mathtt{A[s',i]} \\cdot \\mathtt{B[i,o_j]} \\\\\n",
        "               &=  \\max_{s'=1}^N\n",
        "                   P(o_1,\\ldots, o_j, q_1, \\ldots, q_{j-1}, q_j=i \\mid A, B, \\pi)\n",
        "                   P(q_j=i \\mid q_{j-1}=s') P(o_j \\mid q_j=i)\n",
        "        In words, ``viterbi[i,j]`` is the weighted sum of the values computed\n",
        "        on the previous timestep. The weight on each value is the product of\n",
        "        the probability of transitioning from that state to state `i` and the\n",
        "        probability of emitting observation `j` in state `i`.\n",
        "        To compute the most probable state sequence we maintain a second\n",
        "        trellis, ``back_pointer``, whose `i`, `j` entry contains the value of the\n",
        "        latent state at timestep `j-1` that is most likely to lead to latent\n",
        "        state `i` at timestep `j`.\n",
        "        When we have completed the ``viterbi`` and ``back_pointer`` trellises for\n",
        "        all `T` timseteps/observations, we greedily move backwards through the\n",
        "        ``back_pointer`` trellis to construct the best path for the full\n",
        "        sequence of observations.\n",
        "        Parameters\n",
        "        ----------\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(T,)`\n",
        "            An observation sequence of length `T`.\n",
        "        Returns\n",
        "        -------\n",
        "        best_path : list of length `T`\n",
        "            The most probable sequence of latent states for observations `O`.\n",
        "        best_path_prob : float\n",
        "            The probability of the latent state sequence in `best_path` under\n",
        "            the HMM.\n",
        "        \"\"\"\n",
        "        P = self.parameters\n",
        "        N = self.derived_variables[\"N\"]\n",
        "        eps = self.hyperparameters[\"eps\"]\n",
        "        A, B, pi = P[\"A\"], P[\"B\"], P[\"pi\"]\n",
        "\n",
        "        if O.ndim == 1:\n",
        "            O = O.reshape(1, -1)  # noqa: E741\n",
        "\n",
        "        # number of observations in each sequence\n",
        "        T = O.shape[1]\n",
        "\n",
        "        # number of training sequences\n",
        "        I = O.shape[0]  # noqa: E741\n",
        "        if I != 1:  # noqa: E741\n",
        "            raise ValueError(\"Can only decode a single sequence (O.shape[0] must be 1)\")\n",
        "\n",
        "        # initialize the viterbi and back_pointer matrices\n",
        "        viterbi = np.zeros((N, T))\n",
        "        back_pointer = np.zeros((N, T)).astype(int)\n",
        "\n",
        "        ot = O[0, 0]\n",
        "        for s in range(N):\n",
        "            back_pointer[s, 0] = 0\n",
        "            viterbi[s, 0] = np.log(pi[s] + eps) + np.log(B[s, ot] + eps)\n",
        "\n",
        "        for t in range(1, T):\n",
        "            ot = O[0, t]\n",
        "            for s in range(N):\n",
        "                seq_probs = [\n",
        "                    viterbi[s_, t - 1] + np.log(A[s_, s] + eps) + np.log(B[s, ot] + eps)\n",
        "                    for s_ in range(N)\n",
        "                ]\n",
        "\n",
        "                viterbi[s, t] = np.max(seq_probs)\n",
        "                back_pointer[s, t] = np.argmax(seq_probs)\n",
        "\n",
        "        best_path_log_prob = viterbi[:, T - 1].max()\n",
        "\n",
        "        # backtrack through the trellis to get the most likely sequence of\n",
        "        # latent states\n",
        "        pointer = viterbi[:, T - 1].argmax()\n",
        "        best_path = [pointer]\n",
        "        for t in reversed(range(1, T)):\n",
        "            pointer = back_pointer[pointer, t]\n",
        "            best_path.append(pointer)\n",
        "        best_path = best_path[::-1]\n",
        "\n",
        "        return best_path, best_path_log_prob\n",
        "\n",
        "    def _forward(self, Obs):\n",
        "        r\"\"\"\n",
        "        Computes the forward probability trellis for an HMM parameterized by\n",
        "        :math:`(A, B, \\pi)`.\n",
        "        Notes\n",
        "        -----\n",
        "        The forward trellis (sometimes referred to as `alpha` in the HMM\n",
        "        literature), is a 2D array where entry `i`, `j` represents the probability\n",
        "        under the HMM of being in latent state `i` after seeing the first `j`\n",
        "        observations:\n",
        "        .. math::\n",
        "            \\mathtt{forward[i,j]} =\n",
        "                P(o_1, \\ldots, o_j, q_j=i \\mid A, B, \\pi)\n",
        "        Here :math:`q_j = i` indicates that the hidden state at time `j` is of\n",
        "        type `i`.\n",
        "        The DP step is::\n",
        "        .. math::\n",
        "            forward[i,j] &=\n",
        "                \\sum_{s'=1}^N forward[s',j-1] \\times A[s',i] \\times B[i,o_j] \\\\\n",
        "                &= \\sum_{s'=1}^N P(o_1, \\ldots, o_{j-1}, q_{j-1}=s' \\mid A, B, \\pi)\n",
        "                    \\times P(q_j=i \\mid q_{j-1}=s') \\times P(o_j \\mid q_j=i)\n",
        "        In words, ``forward[i,j]`` is the weighted sum of the values computed\n",
        "        on the previous timestep. The weight on each previous state value is\n",
        "        the product of the probability of transitioning from that state to\n",
        "        state `i` and the probability of emitting observation `j` in state `i`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        Obs : :py:class:`ndarray <numpy.ndarray>` of shape `(T,)`\n",
        "            An observation sequence of length `T`.\n",
        "        Returns\n",
        "        -------\n",
        "        forward : :py:class:`ndarray <numpy.ndarray>` of shape `(N, T)`\n",
        "            The forward trellis.\n",
        "        \"\"\"\n",
        "        P = self.parameters\n",
        "        N = self.derived_variables[\"N\"]\n",
        "        eps = self.hyperparameters[\"eps\"]\n",
        "        A, B, pi = P[\"A\"], P[\"B\"], P[\"pi\"]\n",
        "\n",
        "        T = Obs.shape[0]\n",
        "\n",
        "        # initialize the forward probability matrix\n",
        "        forward = np.zeros((N, T))\n",
        "\n",
        "        ot = Obs[0]\n",
        "        for s in range(N):\n",
        "            forward[s, 0] = np.log(pi[s] + eps) + np.log(B[s, ot] + eps)\n",
        "\n",
        "        for t in range(1, T):\n",
        "            ot = Obs[t]\n",
        "            for s in range(N):\n",
        "                forward[s, t] = logsumexp(\n",
        "                    [\n",
        "                        forward[s_, t - 1]\n",
        "                        + np.log(A[s_, s] + eps)\n",
        "                        + np.log(B[s, ot] + eps)\n",
        "                        for s_ in range(N)\n",
        "                    ]  # noqa: C812\n",
        "                )\n",
        "        return forward\n",
        "\n",
        "    def _backward(self, Obs):\n",
        "        r\"\"\"\n",
        "        Compute the backward probability trellis for an HMM parameterized by\n",
        "        :math:`(A, B, \\pi)`.\n",
        "        Notes\n",
        "        -----\n",
        "        The backward trellis (sometimes referred to as `beta` in the HMM\n",
        "        literature), is a 2D array where entry `i`,`j` represents the probability\n",
        "        of seeing the observations from time `j+1` onward given that the HMM is\n",
        "        in state `i` at time `j`\n",
        "        .. math::\n",
        "            \\mathtt{backward[i,j]} = P(o_{j+1},o_{j+2},\\ldots,o_T \\mid q_j=i,A,B,\\pi)\n",
        "        Here :math:`q_j = i` indicates that the hidden state at time `j` is of type `i`.\n",
        "        The DP step is::\n",
        "            backward[i,j] &=\n",
        "                \\sum_{s'=1}^N backward[s',j+1] \\times A[i, s'] \\times B[s',o_{j+1}] \\\\\n",
        "                &= \\sum_{s'=1}^N P(o_{j+1}, o_{j+2}, \\ldots, o_T \\mid q_j=i, A, B, pi)\n",
        "                    \\times P(q_{j+1}=s' \\mid q_{j}=i) \\times P(o_{j+1} \\mid q_{j+1}=s')\n",
        "        In words, ``backward[i,j]`` is the weighted sum of the values computed\n",
        "        on the following timestep. The weight on each state value from the\n",
        "        `j+1`'th timestep is the product of the probability of transitioning from\n",
        "        state i to that state and the probability of emitting observation `j+1`\n",
        "        from that state.\n",
        "        Parameters\n",
        "        ----------\n",
        "        Obs : :py:class:`ndarray <numpy.ndarray>` of shape `(T,)`\n",
        "            A single observation sequence of length `T`.\n",
        "        Returns\n",
        "        -------\n",
        "        backward : :py:class:`ndarray <numpy.ndarray>` of shape `(N, T)`\n",
        "            The backward trellis.\n",
        "        \"\"\"\n",
        "        P = self.parameters\n",
        "        A, B = P[\"A\"], P[\"B\"]\n",
        "        N = self.derived_variables[\"N\"]\n",
        "        eps = self.hyperparameters[\"eps\"]\n",
        "\n",
        "        T = Obs.shape[0]\n",
        "\n",
        "        # initialize the backward trellis\n",
        "        backward = np.zeros((N, T))\n",
        "\n",
        "        for s in range(N):\n",
        "            backward[s, T - 1] = 0\n",
        "\n",
        "        for t in reversed(range(T - 1)):\n",
        "            ot1 = Obs[t + 1]\n",
        "            for s in range(N):\n",
        "                backward[s, t] = logsumexp(\n",
        "                    [\n",
        "                        np.log(A[s, s_] + eps)\n",
        "                        + np.log(B[s_, ot1] + eps)\n",
        "                        + backward[s_, t + 1]\n",
        "                        for s_ in range(N)\n",
        "                    ]  # noqa: C812\n",
        "                )\n",
        "        return backward\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        P = self.parameters\n",
        "        A, B, pi = P[\"A\"], P[\"B\"], P[\"pi\"]\n",
        "        N, V = self.derived_variables[\"N\"], self.derived_variables[\"V\"]\n",
        "\n",
        "        # Uniform initialization of prior over latent states\n",
        "        if pi is None:\n",
        "            pi = np.ones(N)\n",
        "            pi = pi / pi.sum()\n",
        "\n",
        "        # Uniform initialization of A\n",
        "        if A is None:\n",
        "            A = np.ones((N, N))\n",
        "            A = A / A.sum(axis=1)[:, None]\n",
        "\n",
        "        # Random initialization of B\n",
        "        if B is None:\n",
        "            B = np.random.rand(N, V)\n",
        "            B = B / B.sum(axis=1)[:, None]\n",
        "\n",
        "        P[\"A\"], P[\"B\"], P[\"pi\"] = A, B, pi\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        O,\n",
        "        latent_state_types,\n",
        "        observation_types,\n",
        "        pi=None,\n",
        "        tol=1e-5,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Given an observation sequence `O` and the set of possible latent states,\n",
        "        learn the MLE HMM parameters `A` and `B`.\n",
        "        Notes\n",
        "        -----\n",
        "        Model fitting is done iterativly using the Baum-Welch/Forward-Backward\n",
        "        algorithm, a special case of the EM algorithm.\n",
        "        We begin with an intial estimate for the transition (`A`) and emission\n",
        "        (`B`) matrices and then use these to derive better and better estimates\n",
        "        by computing the forward probability for an observation and then\n",
        "        dividing that probability mass among all the paths that contributed to\n",
        "        it.\n",
        "        Parameters\n",
        "        ----------\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(I, T)`\n",
        "            The set of `I` training observations, each of length `T`.\n",
        "        latent_state_types : list of length `N`\n",
        "            The collection of valid latent states.\n",
        "        observation_types : list of length `V`\n",
        "            The collection of valid observation states.\n",
        "        pi : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
        "            The prior probability of each latent state. If None, assume each\n",
        "            latent state is equally likely a priori. Default is None.\n",
        "        tol : float\n",
        "            The tolerance value. If the difference in log likelihood between\n",
        "            two epochs is less than this value, terminate training. Default is\n",
        "            1e-5.\n",
        "        verbose : bool\n",
        "            Print training stats after each epoch. Default is True.\n",
        "        Returns\n",
        "        -------\n",
        "        A : :py:class:`ndarray <numpy.ndarray>` of shape `(N, N)`\n",
        "            The estimated transition matrix.\n",
        "        B : :py:class:`ndarray <numpy.ndarray>` of shape `(N, V)`\n",
        "            The estimated emission matrix.\n",
        "        pi : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
        "            The estimated prior probabilities of each latent state.\n",
        "        \"\"\"\n",
        "        # observations\n",
        "        if O.ndim == 1:\n",
        "            O = O.reshape(1, -1)  # noqa: E741\n",
        "\n",
        "        # number of training examples (I) and their lengths (T)\n",
        "        I, T = O.shape\n",
        "\n",
        "        # number of types of observation\n",
        "        self.derived_variables[\"V\"] = len(observation_types)\n",
        "\n",
        "        # number of latent state types\n",
        "        self.derived_variables[\"N\"] = len(latent_state_types)\n",
        "\n",
        "        self._initialize_parameters()\n",
        "\n",
        "        P = self.parameters\n",
        "\n",
        "        # iterate E and M steps until convergence criteria is met\n",
        "        step, delta = 0, np.inf\n",
        "        ll_prev = np.sum([self.log_likelihood(o) for o in O])\n",
        "\n",
        "        while delta > tol:\n",
        "            gamma, xi, phi = self._E_step(O)\n",
        "            P[\"A\"], P[\"B\"], P[\"pi\"] = self._M_step(O, gamma, xi, phi)\n",
        "            ll = np.sum([self.log_likelihood(o) for o in O])\n",
        "            delta = ll - ll_prev\n",
        "            ll_prev = ll\n",
        "            step += 1\n",
        "\n",
        "            if verbose:\n",
        "                fstr = \"[Epoch {}] LL: {:.3f} Delta: {:.5f}\"\n",
        "                print(fstr.format(step, ll_prev, delta))\n",
        "\n",
        "        #  return A, B, pi\n",
        "\n",
        "    def _E_step(self, O):\n",
        "        r\"\"\"\n",
        "        Run a single E-step update for the Baum-Welch/Forward-Backward\n",
        "        algorithm. This step estimates ``xi`` and ``gamma``, the excepted\n",
        "        state-state transition counts and the expected state-occupancy counts,\n",
        "        respectively.\n",
        "        ``xi[i,j,k]`` gives the probability of being in state `i` at time `k`\n",
        "        and state `j` at time `k+1` given the observed sequence `O` and the\n",
        "        current estimates for transition (`A`) and emission (`B`) matrices::\n",
        "        .. math::\n",
        "            xi[i,j,k] &= P(q_k=i,q_{k+1}=j \\mid O,A,B,pi) \\\\\n",
        "                      &= \\frac{\n",
        "                            P(q_k=i,q_{k+1}=j,O \\mid A,B,pi)\n",
        "                         }{P(O \\mid A,B,pi)} \\\\\n",
        "                      &= \\frac{\n",
        "                            P(o_1,o_2,\\ldots,o_k,q_k=i \\mid A,B,pi) \\times\n",
        "                            P(q_{k+1}=j \\mid q_k=i) \\times\n",
        "                            P(o_{k+1} \\mid q_{k+1}=j) \\times\n",
        "                            P(o_{k+2},o_{k+3},\\ldots,o_T \\mid q_{k+1}=j,A,B,pi)\n",
        "                         }{P(O \\mid A,B,pi)} \\\\\n",
        "                      &= \\frac{\n",
        "                            \\mathtt{fwd[j, k] * self.A[j, i] *\n",
        "                            self.B[i, o_{k+1}] * bwd[i, k + 1]}\n",
        "                         }{\\mathtt{fwd[:, T].sum()}}\n",
        "        The expected number of transitions from state `i` to state `j` across the\n",
        "        entire sequence is then the sum over all timesteps: ``xi[i,j,:].sum()``.\n",
        "        ``gamma[i,j]`` gives the probability of being in state `i` at time `j`\n",
        "        .. math:: \\mathtt{gamma[i,j]} = P(q_j = i \\mid O, A, B, \\pi)\n",
        "        Parameters\n",
        "        ----------\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(I, T)`\n",
        "            The set of `I` training observations, each of length `T`.\n",
        "        Returns\n",
        "        -------\n",
        "        gamma : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N, T)`\n",
        "            The estimated state-occupancy count matrix.\n",
        "        xi : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N, N, T)`\n",
        "            The estimated state-state transition count matrix.\n",
        "        phi : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N)`\n",
        "            The estimated prior counts for each latent state.\n",
        "        \"\"\"\n",
        "        I, T = O.shape\n",
        "        P = self.parameters\n",
        "        A, B = P[\"A\"], P[\"B\"]\n",
        "        N = self.derived_variables[\"N\"]\n",
        "        eps = self.hyperparameters[\"eps\"]\n",
        "\n",
        "        phi = np.zeros((I, N))\n",
        "        gamma = np.zeros((I, N, T))\n",
        "        xi = np.zeros((I, N, N, T))\n",
        "\n",
        "        for i in range(I):\n",
        "            Obs = O[i, :]\n",
        "            fwd = self._forward(Obs)\n",
        "            bwd = self._backward(Obs)\n",
        "            log_likelihood = logsumexp(fwd[:, T - 1])\n",
        "\n",
        "            t = T - 1\n",
        "            for si in range(N):\n",
        "                gamma[i, si, t] = fwd[si, t] + bwd[si, t] - log_likelihood\n",
        "                phi[i, si] = fwd[si, 0] + bwd[si, 0] - log_likelihood\n",
        "\n",
        "            for t in range(T - 1):\n",
        "                ot1 = Obs[t + 1]\n",
        "                for si in range(N):\n",
        "                    gamma[i, si, t] = fwd[si, t] + bwd[si, t] - log_likelihood\n",
        "                    for sj in range(N):\n",
        "                        xi[i, si, sj, t] = (\n",
        "                            fwd[si, t]\n",
        "                            + np.log(A[si, sj] + eps)\n",
        "                            + np.log(B[sj, ot1] + eps)\n",
        "                            + bwd[sj, t + 1]\n",
        "                            - log_likelihood\n",
        "                        )\n",
        "\n",
        "        return gamma, xi, phi\n",
        "\n",
        "    def _M_step(self, O, gamma, xi, phi):\n",
        "        \"\"\"\n",
        "        Run a single M-step update for the Baum-Welch/Forward-Backward\n",
        "        algorithm.\n",
        "        Parameters\n",
        "        ----------\n",
        "        O : :py:class:`ndarray <numpy.ndarray>` of shape `(I, T)`\n",
        "            The set of `I` training observations, each of length `T`.\n",
        "        gamma : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N, T)`\n",
        "            The estimated state-occupancy count matrix.\n",
        "        xi : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N, N, T)`\n",
        "            The estimated state-state transition count matrix.\n",
        "        phi : :py:class:`ndarray <numpy.ndarray>` of shape `(I, N)`\n",
        "            The estimated starting count matrix for each latent state.\n",
        "        Returns\n",
        "        -------\n",
        "        A : :py:class:`ndarray <numpy.ndarray>` of shape `(N, N)`\n",
        "            The estimated transition matrix.\n",
        "        B : :py:class:`ndarray <numpy.ndarray>` of shape `(N, V)`\n",
        "            The estimated emission matrix.\n",
        "        pi : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
        "            The estimated prior probabilities for each latent state.\n",
        "        \"\"\"\n",
        "        I, T = O.shape\n",
        "        P = self.parameters\n",
        "        DV = self.derived_variables\n",
        "        eps = self.hyperparameters[\"eps\"]\n",
        "\n",
        "        N, V = DV[\"N\"], DV[\"V\"]\n",
        "        A, B, pi = P[\"A\"], P[\"B\"], P[\"pi\"]\n",
        "\n",
        "        # initialize the estimated transition (A) and emission (B) matrices\n",
        "        A = np.zeros((N, N))\n",
        "        B = np.zeros((N, V))\n",
        "        pi = np.zeros(N)\n",
        "\n",
        "        count_gamma = np.zeros((I, N, V))\n",
        "        count_xi = np.zeros((I, N, N))\n",
        "\n",
        "        for i in range(I):\n",
        "            Obs = O[i, :]\n",
        "            for si in range(N):\n",
        "                for vk in range(V):\n",
        "                    if not (Obs == vk).any():\n",
        "                        count_gamma[i, si, vk] = np.log(eps)\n",
        "                    else:\n",
        "                        count_gamma[i, si, vk] = logsumexp(gamma[i, si, Obs == vk])\n",
        "\n",
        "                for sj in range(N):\n",
        "                    count_xi[i, si, sj] = logsumexp(xi[i, si, sj, :])\n",
        "\n",
        "        pi = logsumexp(phi, axis=0) - np.log(I + eps)\n",
        "        np.testing.assert_almost_equal(np.exp(pi).sum(), 1)\n",
        "\n",
        "        for si in range(N):\n",
        "            for vk in range(V):\n",
        "                B[si, vk] = logsumexp(count_gamma[:, si, vk]) - logsumexp(\n",
        "                    count_gamma[:, si, :]  # noqa: C812\n",
        "                )\n",
        "\n",
        "            for sj in range(N):\n",
        "                A[si, sj] = logsumexp(count_xi[:, si, sj]) - logsumexp(\n",
        "                    count_xi[:, si, :]  # noqa: C812\n",
        "                )\n",
        "\n",
        "            np.testing.assert_almost_equal(np.exp(A[si, :]).sum(), 1)\n",
        "            np.testing.assert_almost_equal(np.exp(B[si, :]).sum(), 1)\n",
        "        return np.exp(A), np.exp(B), np.exp(pi)"
      ],
      "metadata": {
        "id": "FIcoCpjB8Io8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the prior probabilities of the 2 states ('-' -> 1 or 'H' -> 0)\n",
        "dct = pre.label_dct\n",
        "hidden_states = pre.flatY\n",
        "\n",
        "count_zero = hidden_states.count(0)\n",
        "count_one = hidden_states.count(1)\n",
        "\n",
        "pi = np.array([count_zero/len(hidden_states), count_one/len(hidden_states)])\n",
        "pi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxC0gEeR9zdX",
        "outputId": "1637d1be-8772-47b1-e1fd-399ccd58b60c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.39728068, 0.60271932])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transition_matrix(arr):\n",
        "    M = np.zeros(shape=(max(arr) + 1, max(arr) + 1))\n",
        "    for (i, j) in zip(arr, arr[1:]):\n",
        "        M[i, j] += 1\n",
        "    return (M.T / M.sum(axis=1)).T"
      ],
      "metadata": {
        "id": "CKGlKdzN90hA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = transition_matrix(np.array(pre.flatY))\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIgyP-uD-FkY",
        "outputId": "b0f1223c-a6ad-4251-b95b-064081c1dea7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.90451578, 0.09548422],\n",
              "       [0.06293824, 0.93706176]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the observable emission matrix\n",
        "\n",
        "#determine the indices for helix and non-helix\n",
        "h_inds = np.where(np.array(pre.flatY) == 0)[0]\n",
        "nh_inds = np.where(np.array(pre.flatY) == 1)[0]\n",
        "\n",
        "#get all the corresponding 'observations' or AAs for when in a helix or not\n",
        "obs_h = np.array(pre.flatX)[h_inds]\n",
        "obs_nh = np.array(pre.flatX)[nh_inds]\n",
        "\n",
        "#get the counts for each of these amino acids in both states\n",
        "aa_counts_h = np.bincount(obs_h)\n",
        "aa_counts_nh = np.bincount(obs_nh)\n",
        "\n",
        "#turn into emission probabilities matrix (probabilities of each amino acid given a helix or not)\n",
        "p_aa_h = aa_counts_h/len(h_inds)\n",
        "p_aa_nh = aa_counts_nh/len(nh_inds)\n",
        "\n",
        "theta = np.array([p_aa_h, p_aa_nh])\n",
        "theta"
      ],
      "metadata": {
        "id": "cIH3dlut-LyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialHMM(A=A, B=theta, pi=pi)"
      ],
      "metadata": {
        "id": "OrRtMTci9nfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, prob = model.decode(np.array(pre.flatX))"
      ],
      "metadata": {
        "id": "0hBRum8a-PZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training Accuracy: {(np.array(obs) == np.array(pre.flatY)).mean()}')"
      ],
      "metadata": {
        "id": "0wYMiSPl-SIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing for LSTM"
      ],
      "metadata": {
        "id": "0BzUQrUp-aOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams = np.array([[seq[i:i+3] for i in range(len(seq))] for seq in seqs], dtype=object) # creat ngrams from input training data\n",
        "\n",
        "def tokenize(arr):\n",
        "  set_ngrams = set([])\n",
        "  for lst in arr:\n",
        "    for ngram in lst:\n",
        "      set_ngrams.add(ngram.lower())\n",
        "\n",
        "  ngram2idx = {ng: i+1 for i, ng in enumerate(list(set_ngrams))}\n",
        "  return ngram2idx\n",
        "\n",
        "ngram_dict = tokenize(ngrams)\n",
        "\n",
        "def transform(arr, dict):\n",
        "  seqs2int = []\n",
        "  for lst in arr:\n",
        "    ints = []\n",
        "    for i in lst:\n",
        "      ints.append(dict[i.lower()])\n",
        "    seqs2int.append(ints)\n",
        "  return seqs2int\n",
        "\n",
        "train_X = transform(ngrams, ngram_dict) # now all the ngrams are tokenized"
      ],
      "metadata": {
        "id": "e-lc2s77-eEQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to pad or truncate list to specific length\n",
        "pad = lambda a,i : a[0:i] if len(a) > i else a + [0] * (i-len(a))\n",
        "\n",
        "padded = []\n",
        "\n",
        "for l in train_X:\n",
        "  pl = pad(l, 1419)\n",
        "  padded.append(pl)\n",
        "\n",
        "np_train_X = np.array(padded)"
      ],
      "metadata": {
        "id": "CYszOVZ1-n33"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_tokenizer(lst): # pass in a list of the labels\n",
        "  set_labels = set([])\n",
        "  for l in lst:\n",
        "    for seq in l:\n",
        "      for char in list(seq):\n",
        "        set_labels.add(char.lower())\n",
        "  label2idx = {l: i+1 for i, l in enumerate(list(set_labels))}\n",
        "  return label2idx \n",
        "\n",
        "label_dict = char_tokenizer(labels) # fit the labels\n",
        "\n",
        "train_y = transform(labels, label_dict) #labels are now tokenized\n",
        "\n",
        "padded_y = []\n",
        "for l in train_y:\n",
        "  pl = pad(l, 1419)\n",
        "  padded_y.append(pl)\n",
        "\n",
        "np_train_y = np.array(padded_y)"
      ],
      "metadata": {
        "id": "E53sMIUZ-w25"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oh_y_train = np.eye(3)[np_train_y] # 3 categories since '-, H, 0' for the padding"
      ],
      "metadata": {
        "id": "sV32lsyP-2o6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(np_train_X, oh_y_train, \n",
        "                                                    test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "dKLOh7ib-5c0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#append missing value\n",
        "ngram_dict['mwm'] = 8413"
      ],
      "metadata": {
        "id": "7518ycbV_BA6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haCMCLmR_DCZ",
        "outputId": "9ae72265-6c0a-4b4c-ef30-476dd11bf9f9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994, 1419), (3994, 1419, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM Model"
      ],
      "metadata": {
        "id": "Zx5Kq0tN_HeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, TimeDistributed\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "_YYPu56S_RJO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = len(ngram_dict) + 1 #plus 1 for the padding token 0\n",
        "max_length = 1419\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_shape=(max_length,), input_dim=n_words, output_dim=4))\n",
        "model.add(LSTM(4, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(3, activation='softmax')))"
      ],
      "metadata": {
        "id": "3HQz4gRS_aT9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to calculate accuracy that ignores the padding\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "def no_pad_acc(y_true, y_pred):\n",
        "  y = tf.argmax(y_true, axis=-1)\n",
        "  y_ = tf.argmax(y_pred, axis=-1)\n",
        "  mask = tf.greater(y, 0)\n",
        "  return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())"
      ],
      "metadata": {
        "id": "8274QRYD_h6o"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.08),\n",
        "              metrics=['accuracy', no_pad_acc])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IZgQy7R_vcv",
        "outputId": "0a282305-a017-4e07-f611-aac4075f7780"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1419, 4)           33656     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1419, 4)           144       \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 1419, 3)          15        \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,815\n",
            "Trainable params: 33,815\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use GPU for this\n",
        "model.fit(X_train,y_train,epochs=6, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "aMiQhXNQ_1Z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}